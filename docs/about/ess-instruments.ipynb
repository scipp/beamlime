{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ESS Instruments\n",
    "\n",
    "``beamlime`` is designed and implemented to support live data reduction at ESS.\n",
    "\n",
    "ESS has various instruments and each of them has different range of computation loads.\n",
    "\n",
    "Here is the plot of ``number of pixel`` and ``event rate`` ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")  # To use ``docs`` as a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess_requirements import ESSInstruments\n",
    "\n",
    "ess_requirements = ESSInstruments()\n",
    "ess_requirements.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "There is a set of benchmark results we have collected with dummy workflow in various computing environments.\n",
    "\n",
    "They are collected with the ``benchmarks`` module in ``tests`` package in the repository.\n",
    "\n",
    "And the ``benchmarks`` module also has loading/visualization helpers.\n",
    "\n",
    "Here is a contour performance plot of one of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import collect_reports, merge_measurements\n",
    "from docs.about.data import benchmark_results\n",
    "import json\n",
    "\n",
    "results = benchmark_results()\n",
    "results_map = json.loads(results.read_text())\n",
    "report_map = collect_reports(results_map)\n",
    "df = merge_measurements(report_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten required hardware specs into columns.\n",
    "from environments import BenchmarkEnvironment\n",
    "\n",
    "\n",
    "def retrieve_total_memory(env: BenchmarkEnvironment) -> float:\n",
    "    return env.hardware_spec.total_memory.value\n",
    "\n",
    "\n",
    "def retrieve_cpu_cores(env: BenchmarkEnvironment) -> float:\n",
    "    return env.hardware_spec.cpu_spec.process_cpu_affinity.value\n",
    "\n",
    "\n",
    "df[\"total_memory [GB]\"] = df[\"environment\"].apply(retrieve_total_memory)\n",
    "df[\"cpu_cores\"] = df[\"environment\"].apply(retrieve_cpu_cores)\n",
    "\n",
    "# Fix column names to have proper units.\n",
    "df.rename(\n",
    "    columns={\n",
    "        \"num_pixels\": \"num_pixels [counts]\",\n",
    "        \"num_events\": \"num_events [counts]\",\n",
    "        \"num_frames\": \"num_frames [counts]\",\n",
    "        \"event_rate\": \"event_rate [counts/s]\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipp as sc\n",
    "from scipp.compat.pandas_compat import from_pandas, parse_bracket_header\n",
    "\n",
    "# Convert to scipp dataset.\n",
    "ds: sc.Dataset = from_pandas(\n",
    "    df[df[\"target-name\"] == \"mini_prototype\"].drop(columns=[\"environment\"]),\n",
    "    header_parser=parse_bracket_header,\n",
    "    data_columns=\"time\",\n",
    ")\n",
    "\n",
    "# Derive speed from time measurements and number of frames.\n",
    "ds[\"speed\"] = ds.coords[\"num_frames\"] / ds[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calculations import sample_mean_per_bin, sample_variance_per_bin\n",
    "\n",
    "# Calculate mean and variance per bin.\n",
    "binned = ds[\"speed\"].group(\"event_rate\", \"num_pixels\", \"cpu_cores\")\n",
    "da = sample_mean_per_bin(binned)\n",
    "da.variances = sample_variance_per_bin(binned).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select measurement with 63 CPU cores.\n",
    "da_63_cores = da[\"cpu_cores\", sc.scalar(63, unit=None)]\n",
    "\n",
    "# Create a meta string with the selected data.\n",
    "df_63_cores = df[df[\"cpu_cores\"] == 63].reset_index(drop=True)\n",
    "df_63_cores_envs: BenchmarkEnvironment = df_63_cores[\"environment\"][0]\n",
    "meta_64_cores = [\n",
    "    f\"{ds.coords['target-name'][0].value} \"\n",
    "    f\"of beamlime @ {df_63_cores_envs.git_commit_id[:7]} \",\n",
    "    f\"on {df_63_cores_envs.hardware_spec.operating_system} \"\n",
    "    f\"with {df_63_cores_envs.hardware_spec.total_memory.value} \"\n",
    "    f\"[{df_63_cores_envs.hardware_spec.total_memory.unit}] of memory, \"\n",
    "    f\"{da_63_cores.coords['cpu_cores'].value} CPU cores\",\n",
    "    f\"processing total [{df_63_cores['num_frames [counts]'].min()}, {df_63_cores['num_frames [counts]'].max()}] frames \",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a contour plot.\n",
    "from matplotlib import pyplot as plt\n",
    "from visualize import plot_contourf\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "ctr = plot_contourf(\n",
    "    da_63_cores,\n",
    "    x_coord=\"event_rate\",\n",
    "    y_coord=\"num_pixels\",\n",
    "    fig=fig,\n",
    "    ax=ax,\n",
    "    levels=[2, 4, 8, 14, 32, 64, 128],\n",
    "    extend=\"both\",\n",
    "    colors=[\"gold\", \"yellow\", \"orange\", \"lime\", \"yellowgreen\", \"green\", \"darkgreen\"],\n",
    "    under_color=\"lightgrey\",\n",
    "    over_color=\"darkgreen\",\n",
    ")\n",
    "ess_requirements.plot_boundaries(ax)\n",
    "ess_requirements.configure_full_scale(ax)\n",
    "\n",
    "ax.set_title(\"Beamlime Performance Contour Plot\")\n",
    "ax.annotate(\"14.00 [frame/s]\", (5e4, 9e6), size=10)\n",
    "ax.text(10**4, 10**8, \"\\n\".join(meta_64_cores))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Performance Comparisons\n",
    "\n",
    "We will compare performances of different memory capacity and number of cpu cores.\n",
    "\n",
    "Performance differences are calculated with the following function ``difference``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(da: sc.DataArray, standard_da: sc.DataArray) -> sc.DataArray:\n",
    "    \"\"\"Difference from the standard data array in percent.\"\"\"\n",
    "\n",
    "    return sc.scalar(100, unit=\"%\") * (da - standard_da) / standard_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "More memory capacity did not make any meaningful performance improvement tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import plopp as pp\n",
    "\n",
    "mem_da = ds[\"speed\"].group(\"total_memory\", \"num_pixels\", \"event_rate\", \"cpu_cores\")\n",
    "mem_da = mem_da[\"cpu_cores\", sc.scalar(6, unit=None)].drop_coords(\n",
    "    \"cpu_cores\"\n",
    ")  # Select 6 CPU cores.\n",
    "memory_comparison = mem_da.flatten(\n",
    "    (\"num_pixels\", \"event_rate\"), \"num_pixels_event_rate\"\n",
    ")\n",
    "mean_memory_comparison = sample_mean_per_bin(memory_comparison)\n",
    "mean_memory_comparison.variances = sample_variance_per_bin(memory_comparison).values\n",
    "x_tick_labels = [\n",
    "    f\"{npx=:.0e}\\n{er=:.0e}\"\n",
    "    for er, npx in zip(\n",
    "        mean_memory_comparison.coords[\"event_rate\"].values,\n",
    "        mean_memory_comparison.coords[\"num_pixels\"].values,\n",
    "        strict=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "mean_memory_comparison.coords[\"label\"] = sc.arange(\n",
    "    \"num_pixels_event_rate\", len(x_tick_labels)\n",
    ")\n",
    "standard_mem_speed = mean_memory_comparison[\"total_memory\", 0]\n",
    "lines_per_memory = {\n",
    "    f\"{mem.value} [{mem.unit}]\": difference(\n",
    "        mean_memory_comparison[\"total_memory\", imem], standard_mem_speed\n",
    "    )\n",
    "    for imem, mem in enumerate(mean_memory_comparison.coords[\"total_memory\"])\n",
    "}\n",
    "memory_comparison_line_plot = pp.plot(\n",
    "    lines_per_memory,\n",
    "    coords=[\"label\"],\n",
    "    title=\"Beamlime Performance Comparison per Memory Size\",\n",
    "    figsize=(24, 4),\n",
    "    grid=True,\n",
    ")\n",
    "\n",
    "for i_line, line_name in zip([0, 2], lines_per_memory.keys(), strict=True):\n",
    "    memory_comparison_line_plot.ax.lines[i_line].set_label(line_name)\n",
    "\n",
    "df_mem_comparison = df[df[\"cpu_cores\"] == 6].reset_index(drop=True)\n",
    "df_mem_comparison_env: BenchmarkEnvironment = df_mem_comparison[\"environment\"][0]\n",
    "meta_mem_comparison = [\n",
    "    f\"on {df_mem_comparison_env.hardware_spec.operating_system} \",\n",
    "    f\"{6} CPU cores\",\n",
    "    f\"processing total [{df_mem_comparison['num_frames [counts]'].min()}, \"\n",
    "    f\"{df_mem_comparison['num_frames [counts]'].max()}] frames \",\n",
    "    \"difference=100*(speed-standard_speed)/standard_speed, with standard speed: 67 GB\",\n",
    "]\n",
    "\n",
    "memory_comparison_line_plot.ax.set_xticks(list(range(len(x_tick_labels))))\n",
    "memory_comparison_line_plot.ax.set_xticklabels(x_tick_labels)\n",
    "memory_comparison_line_plot.ax.text(-1, 24, \"\\n\".join(meta_mem_comparison))\n",
    "memory_comparison_line_plot.ax.legend(loc=\"lower left\", title=\"Memory Size [GB]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_comparison_line_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### CPU Cores\n",
    "\n",
    "More CPU cores showed improved performance for most cases, especially bigger number of events.\n",
    "\n",
    "It was expected due to multi-threaded computing of scipp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import plopp as pp\n",
    "\n",
    "cpu_da = ds[\"speed\"].group(\"total_memory\", \"num_pixels\", \"event_rate\", \"cpu_cores\")\n",
    "cpu_da = cpu_da[\"total_memory\", sc.scalar(135, unit=\"GB\")].drop_coords(\n",
    "    \"total_memory\"\n",
    ")  # Select 135 GB.\n",
    "cpu_comparison = cpu_da.flatten((\"num_pixels\", \"event_rate\"), \"num_pixels_event_rate\")\n",
    "mean_cpu_comparison = sample_mean_per_bin(cpu_comparison)\n",
    "mean_cpu_comparison.variances = sample_variance_per_bin(cpu_comparison).values\n",
    "mean_cpu_comparison.coords[\"label\"] = sc.arange(\n",
    "    \"num_pixels_event_rate\", len(x_tick_labels)\n",
    ")\n",
    "\n",
    "standard_cpu_speed = mean_cpu_comparison[\"cpu_cores\", 0]\n",
    "lines_per_cpu = {\n",
    "    f\"{ncpu.value}\": difference(\n",
    "        mean_cpu_comparison[\"cpu_cores\", icpu], standard_cpu_speed\n",
    "    )\n",
    "    for icpu, ncpu in enumerate(mean_cpu_comparison.coords[\"cpu_cores\"])\n",
    "}\n",
    "cpu_comparison_line_plot = pp.plot(\n",
    "    lines_per_cpu,\n",
    "    coords=[\"label\"],\n",
    "    title=\"Beamlime Performance Comparison per Number of CPU Cores\",\n",
    "    figsize=(24, 4),\n",
    "    grid=True,\n",
    "    # norm='log',\n",
    ")\n",
    "\n",
    "for i_line, line_name in enumerate(lines_per_cpu.keys()):\n",
    "    cpu_comparison_line_plot.ax.lines[i_line * 2].set_label(line_name)\n",
    "\n",
    "df_cpu_comparison = df[df[\"total_memory [GB]\"] == 135].reset_index(drop=True)\n",
    "df_cpu_comparison_env: BenchmarkEnvironment = df_cpu_comparison[\"environment\"][0]\n",
    "meta_cpu_comparison = [\n",
    "    f\"on {df_cpu_comparison_env.hardware_spec.operating_system} \",\n",
    "    f\"{6} CPU cores\",\n",
    "    f\"processing total [{df_cpu_comparison['num_frames [counts]'].min()}, \"\n",
    "    f\"{df_cpu_comparison['num_frames [counts]'].max()}] frames \",\n",
    "]\n",
    "\n",
    "cpu_comparison_line_plot.ax.set_xticks(list(range(len(x_tick_labels))))\n",
    "cpu_comparison_line_plot.ax.set_xticklabels(x_tick_labels)\n",
    "cpu_comparison_line_plot.ax.text(-1, 900, \"\\n\".join(meta_cpu_comparison))\n",
    "cpu_comparison_line_plot.ax.legend(loc=\"lower left\", title=\"CPU Cores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_comparison_line_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lime-dev-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
