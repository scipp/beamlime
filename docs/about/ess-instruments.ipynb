{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a09e19d",
   "metadata": {},
   "source": [
    "# ESS Instruments\n",
    "\n",
    "``beamlime`` is designed and implemented to support live data reduction at ESS.\n",
    "\n",
    "ESS has various instruments and each of them has different range of computation loads.\n",
    "\n",
    "Here is the plot of ``number of pixel`` and ``event rate`` ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc772aa",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')  # To use ``docs`` as a package.\n",
    "sys.path.append('../../tests/helpers/')  # To use ``tests`` as a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.ess_requirements import ESSInstruments\n",
    "\n",
    "ess_requirements = ESSInstruments()\n",
    "ess_requirements.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2d514",
   "metadata": {},
   "source": [
    "There is a set of benchmark results we have collected with dummy workflow in various computing environments.\n",
    "\n",
    "They are collected with the ``benchmarks`` module in ``tests`` package in the repository.\n",
    "\n",
    "And the ``benchmarks`` module also has loading/visualization helpers.\n",
    "\n",
    "Here is a contour performance plot of one of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73befafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.environments import env_providers\n",
    "from tests.benchmarks.loader import loading_providers, MergedMeasurementsDF, ResultMap\n",
    "from beamlime.constructors import Factory\n",
    "from docs.about.data import benchmark_results\n",
    "import json\n",
    "\n",
    "results = benchmark_results()\n",
    "results_map = json.loads(results.read_text())\n",
    "factory = Factory(loading_providers, env_providers)\n",
    "with factory.constant_provider(ResultMap, results_map):\n",
    "    df = factory[MergedMeasurementsDF]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten required hardware specs into columns.\n",
    "from tests.benchmarks.environments import BenchmarkEnvironment\n",
    "\n",
    "def retrieve_total_memory(env: BenchmarkEnvironment) -> float:\n",
    "    return env.hardware_spec.total_memory.value\n",
    "\n",
    "def retrieve_cpu_cores(env: BenchmarkEnvironment) -> float:\n",
    "    return env.hardware_spec.cpu_spec.process_cpu_affinity.value\n",
    "\n",
    "df[\"total_memory [GB]\"] = df['environment'].apply(retrieve_total_memory)\n",
    "df[\"cpu_cores\"] = df['environment'].apply(retrieve_cpu_cores)\n",
    "\n",
    "# Fix column names to have proper units.\n",
    "df.rename(\n",
    "    columns={\n",
    "        'num_pixels': 'num_pixels [counts]',\n",
    "        'num_events': 'num_events [counts]',\n",
    "        'num_frames': 'num_frames [counts]',\n",
    "        'event_rate': 'event_rate [counts/s]',\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipp as sc\n",
    "from scipp.compat.pandas_compat import from_pandas, parse_bracket_header\n",
    "\n",
    "# Convert to scipp dataset.\n",
    "ds: sc.Dataset = from_pandas(\n",
    "    df[df['target-name'] == 'mini_prototype'].drop(columns=['environment']),\n",
    "    header_parser=parse_bracket_header,\n",
    "    data_columns='time'\n",
    ")\n",
    "\n",
    "# Derive speed from time measurements and number of frames.\n",
    "ds['speed'] = ds.coords['num_frames']/ds['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.calculations import sample_mean_per_bin, sample_variance_per_bin\n",
    "\n",
    "# Calculate mean and variance per bin.\n",
    "binned = ds['speed'].group('event_rate', 'num_pixels', 'cpu_cores')\n",
    "da = sample_mean_per_bin(binned)\n",
    "da.variances = sample_variance_per_bin(binned).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select measurement with 63 CPU cores.\n",
    "da_63_cores = da['cpu_cores', sc.scalar(63, unit=None)]\n",
    "\n",
    "# Create a meta string with the selected data.\n",
    "df_63_cores = df[df['cpu_cores']==63].reset_index(drop=True)\n",
    "df_63_cores_envs: BenchmarkEnvironment = df_63_cores['environment'][0]\n",
    "meta_64_cores = [\n",
    "    f\"{ds.coords['target-name'][0].value} \"\n",
    "    f\"of beamlime @ {df_63_cores_envs.git_commit_id[:7]} \",\n",
    "    f\"on {df_63_cores_envs.hardware_spec.operating_system} \"\n",
    "    f\"with {df_63_cores_envs.hardware_spec.total_memory.value} \"\n",
    "    f\"[{df_63_cores_envs.hardware_spec.total_memory.unit}] of memory, \"\n",
    "    f\"{da_63_cores.coords['cpu_cores'].value} CPU cores\",\n",
    "    f\"processing total [{df_63_cores['num_frames [counts]'].min()}, {df_63_cores['num_frames [counts]'].max()}] frames \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a contour plot.\n",
    "from matplotlib import pyplot as plt\n",
    "from tests.benchmarks.visualize import plot_contourf\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "ctr = plot_contourf(da_63_cores,\n",
    "    x_coord='event_rate',\n",
    "    y_coord='num_pixels',\n",
    "    fig=fig,\n",
    "    ax=ax,\n",
    "    levels=[2, 4, 8, 14, 32, 64, 128],\n",
    "    extend='both',\n",
    "    colors=['gold', 'yellow', 'orange', 'lime', 'yellowgreen', 'green', 'darkgreen'],\n",
    "    under_color='lightgrey',\n",
    "    over_color='darkgreen',\n",
    ")\n",
    "ess_requirements.plot_boundaries(ax)\n",
    "ess_requirements.configure_full_scale(ax)\n",
    "\n",
    "ax.set_title('Beamlime Performance Contour Plot')\n",
    "ax.annotate('14.00 [frame/s]', (5e4, 9e6), size=10)\n",
    "ax.text(10**4, 10**8, '\\n'.join(meta_64_cores))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabfc0b3",
   "metadata": {},
   "source": [
    "## Performance Comparisons\n",
    "\n",
    "We will compare performances of different memory capacity and number of cpu cores.\n",
    "\n",
    "Performance differences are calculated with the following function ``difference``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e05f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(da: sc.DataArray, standard_da: sc.DataArray) -> sc.DataArray:\n",
    "    \"\"\"Difference from the standard data array in percent.\"\"\"\n",
    "\n",
    "    return sc.scalar(100, unit='%')*(da - standard_da) / standard_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8cb8a7",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "More memory capacity did not make any meaningful performance improvement tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cbff1e",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import plopp as pp\n",
    "\n",
    "mem_da = ds['speed'].group('total_memory', 'num_pixels', 'event_rate', 'cpu_cores')\n",
    "mem_da = mem_da['cpu_cores', sc.scalar(6, unit=None)].drop_coords('cpu_cores')  # Select 6 CPU cores.\n",
    "memory_comparison = mem_da.flatten(('num_pixels', 'event_rate'), 'num_pixels_event_rate')\n",
    "mean_memory_comparison = sample_mean_per_bin(memory_comparison)\n",
    "mean_memory_comparison.variances = sample_variance_per_bin(memory_comparison).values\n",
    "x_tick_labels = [\n",
    "    f\"{npx=:.0e}\\n{er=:.0e}\"\n",
    "    for er, npx\n",
    "    in zip(\n",
    "        mean_memory_comparison.coords['event_rate'].values,\n",
    "        mean_memory_comparison.coords['num_pixels'].values\n",
    "    )\n",
    "]\n",
    "mean_memory_comparison.coords['label'] = sc.array(dims=['num_pixels_event_rate'], values=x_tick_labels)\n",
    "\n",
    "standard_mem_speed = mean_memory_comparison['total_memory', 0]\n",
    "lines_per_memory = {\n",
    "        f\"{mem.value} [{mem.unit}]\": difference(mean_memory_comparison['total_memory', imem], standard_mem_speed)\n",
    "        for imem, mem in enumerate(mean_memory_comparison.coords['total_memory'])\n",
    "    }\n",
    "memory_comparison_line_plot = pp.plot(\n",
    "    lines_per_memory,\n",
    "    coords=['label'],\n",
    "    title=\"Beamlime Performance Comparison per Memory Size\",\n",
    "    figsize=(24, 4),\n",
    "    grid=True,\n",
    ")\n",
    "\n",
    "for i_line, line_name in zip([0, 2], lines_per_memory.keys()):\n",
    "    memory_comparison_line_plot.ax.lines[i_line].set_label(line_name)\n",
    "\n",
    "df_mem_comparison = df[df['cpu_cores']==6].reset_index(drop=True)\n",
    "df_mem_comparison_env: BenchmarkEnvironment = df_mem_comparison['environment'][0]\n",
    "meta_mem_comparison = [\n",
    "    f\"on {df_mem_comparison_env.hardware_spec.operating_system} \",\n",
    "    f\"{6} CPU cores\",\n",
    "    f\"processing total [{df_mem_comparison['num_frames [counts]'].min()}, \"\n",
    "    f\"{df_mem_comparison['num_frames [counts]'].max()}] frames \",\n",
    "    \"difference=100*(speed-standard_speed)/standard_speed, with standard speed: 67 GB\"\n",
    "]\n",
    "\n",
    "memory_comparison_line_plot.ax.text(-1,  20, '\\n'.join(meta_mem_comparison))\n",
    "memory_comparison_line_plot.ax.legend(loc='lower left', title='Memory Size [GB]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace91b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_comparison_line_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6010d",
   "metadata": {},
   "source": [
    "### CPU Cores\n",
    "\n",
    "More CPU cores showed improved performance for most cases, especially bigger number of events.\n",
    "\n",
    "It was expected due to multi-threaded computing of scipp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38f4aa",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import plopp as pp\n",
    "\n",
    "cpu_da = ds['speed'].group('total_memory', 'num_pixels', 'event_rate', 'cpu_cores')\n",
    "cpu_da = cpu_da['total_memory', sc.scalar(135, unit='GB')].drop_coords('total_memory')  # Select 135 GB.\n",
    "cpu_comparison = cpu_da.flatten(('num_pixels', 'event_rate'), 'num_pixels_event_rate')\n",
    "mean_cpu_comparison = sample_mean_per_bin(cpu_comparison)\n",
    "mean_cpu_comparison.variances = sample_variance_per_bin(cpu_comparison).values\n",
    "mean_cpu_comparison.coords['label'] = sc.array(dims=['num_pixels_event_rate'], values=x_tick_labels)\n",
    "\n",
    "standard_cpu_speed = mean_cpu_comparison['cpu_cores', 0]\n",
    "lines_per_cpu = {\n",
    "        f\"{ncpu.value}\": difference(mean_cpu_comparison['cpu_cores', icpu], standard_cpu_speed)\n",
    "        for icpu, ncpu in enumerate(mean_cpu_comparison.coords['cpu_cores'])\n",
    "    }\n",
    "cpu_comparison_line_plot = pp.plot(\n",
    "    lines_per_cpu,\n",
    "    coords=['label'],\n",
    "    title=\"Beamlime Performance Comparison per Number of CPU Cores\",\n",
    "    figsize=(24, 4),\n",
    "    grid=True,\n",
    "    # norm='log',\n",
    ")\n",
    "\n",
    "for i_line, line_name in enumerate(lines_per_cpu.keys()):\n",
    "    cpu_comparison_line_plot.ax.lines[i_line*2].set_label(line_name)\n",
    "\n",
    "df_cpu_comparison = df[df['total_memory [GB]']==135].reset_index(drop=True)\n",
    "df_cpu_comparison_env: BenchmarkEnvironment = df_cpu_comparison['environment'][0]\n",
    "meta_cpu_comparison = [\n",
    "    f\"on {df_cpu_comparison_env.hardware_spec.operating_system} \",\n",
    "    f\"{6} CPU cores\",\n",
    "    f\"processing total [{df_cpu_comparison['num_frames [counts]'].min()}, \"\n",
    "    f\"{df_cpu_comparison['num_frames [counts]'].max()}] frames \"\n",
    "]\n",
    "\n",
    "cpu_comparison_line_plot.ax.text(-1,  800, '\\n'.join(meta_cpu_comparison))\n",
    "cpu_comparison_line_plot.ax.legend(loc='lower left', title='CPU Cores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a13cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_comparison_line_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d973ee7",
   "metadata": {},
   "source": [
    "### Offline/Online reduction\n",
    "\n",
    "``Online`` data reduction means, it processed chunks of data and merged the result at the end.\n",
    "\n",
    "``Offline`` data redution means, it processed all dataset at once.\n",
    "It was collected via one of benchmark tests in ``workflow_test.py`` under ``tests/prototypes``.\n",
    "\n",
    "This comparison shows the overhead of ``beamlime`` compared to the normal data reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9946706",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Convert to scipp dataset.\n",
    "ds_offline: sc.Dataset = from_pandas(\n",
    "    df[df['target-name'] == 'offline_workflow'].drop(columns=['environment']),\n",
    "    header_parser=parse_bracket_header,\n",
    "    data_columns='time'\n",
    ")\n",
    "\n",
    "# Derive speed from time measurements and number of frames.\n",
    "ds_offline['speed'] = ds_offline.coords['num_frames']/ds_offline['time']\n",
    "\n",
    "def group_and_mean_flatten(da: sc.DataArray, *coords) -> sc.DataArray:\n",
    "    binned = da.group(*coords)\n",
    "    mean_da = sample_mean_per_bin(binned)\n",
    "    mean_da.variances = sample_variance_per_bin(binned).values\n",
    "    return mean_da.flatten(coords, \"_\".join(coords))\n",
    "\n",
    "offline_da = group_and_mean_flatten(ds_offline['speed'], 'num_pixels', 'event_rate')\n",
    "\n",
    "grouped_by_hw_conditions = ds_offline['speed'].group('cpu_cores', 'total_memory')\n",
    "selected = ds['speed'].group(*(coord for coord in grouped_by_hw_conditions.coords.values()))\n",
    "if len(selected) > 1:\n",
    "    raise ValueError(\"There are more than 1 bin in the grouped dataset.\")\n",
    "\n",
    "online_da = group_and_mean_flatten(selected.values[0], 'num_pixels', 'event_rate')\n",
    "\n",
    "for da in (online_da, offline_da):\n",
    "    da.coords['label'] = sc.array(dims=['num_pixels_event_rate'], values=x_tick_labels)\n",
    "\n",
    "onoff_lines = {\n",
    "        label: difference(onoff_da, offline_da)\n",
    "        for label, onoff_da in zip([\"offline\", \"online\"], [offline_da, online_da])\n",
    "    }\n",
    "onoffline_comparison = pp.plot(\n",
    "    onoff_lines,\n",
    "    coords=['label'],\n",
    "    title=\"Beamlime online reduction speed compared to offline reduction.\",\n",
    "    figsize=(24, 4),\n",
    "    grid=True\n",
    ")\n",
    "\n",
    "for i_line, line_name in zip([0, 2], onoff_lines.keys()):\n",
    "    onoffline_comparison.ax.lines[i_line].set_label(line_name)\n",
    "\n",
    "df_offline = df[df['target-name']=='offline_workflow'].reset_index(drop=True)\n",
    "df_offline_env: BenchmarkEnvironment = df_offline['environment'][0]\n",
    "meta_onoff_comparison = [\n",
    "    f\"on {df_offline_env.hardware_spec.operating_system}, \"\n",
    "    f\"{grouped_by_hw_conditions.coords['cpu_cores'][0].value} CPU cores\",\n",
    "    f\"processing total [{df_offline['num_frames [counts]'].min()}, \"\n",
    "    f\"{df_offline['num_frames [counts]'].max()}] frames \",\n",
    "    \"difference=100*(speed-standard_speed)/standard_speed, \\nwith standard speed: Offline\"\n",
    "]\n",
    "\n",
    "onoffline_comparison.ax.text(-1,  12, '\\n'.join(meta_onoff_comparison))\n",
    "onoffline_comparison.ax.legend(loc='center left', title='on/off, Offline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b59615",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffline_comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
