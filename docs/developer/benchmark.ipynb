{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3756777c",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "ESS is built for various types of experiments with multiple instruments using different techniques.\n",
    "\n",
    "Benchmarking was needed to make sure the data reduction frameworks can handle the streaming data in real time.\n",
    "\n",
    "We will monitor the two types of computing costs, ``time`` and ``space(memory)`` of workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4d6e5",
   "metadata": {},
   "source": [
    "## Running Benchmarks\n",
    "\n",
    "Benchmark tests and related tools are collected in ``tests/benchmarks``\n",
    "and ``tests/prototypes`` of the [repository](https://github.com/scipp/beamlime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa96a4",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../../tests/helpers/'))  # To use ``tests`` as a package.\n",
    "                                                             # This path is already included in ``pyproject.toml``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1ea33",
   "metadata": {},
   "source": [
    "### Benchmark Session\n",
    "\n",
    "``BenchmarkSession`` is the entry point to run benchmark tests. <br>\n",
    "\n",
    "``BenchmarkSession`` is a bridge object that connects ``FileManager``, ``BenchmarkReport`` and ``BenchmarkRunner`` instances. <br>\n",
    "Therefore ``session`` carries all other components, ``session.report``, ``session.runner`` and ``session.file_manager`` as its fields. <br>\n",
    "\n",
    "``BenchmarkSession`` is a nested dataclass and it is meant to be built by dependency injection. <br>\n",
    "``create_benchmark_session_factory`` is a helper to build a ``beamlime.constructor.Factory`` <br>\n",
    "with all necessary providers to build ``BenchmarkSession`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import Pretty\n",
    "from tests.benchmarks.runner import create_benchmark_session_factory, BenchmarkSession, AutoSaveFlag\n",
    "\n",
    "benchmark_session_factory = create_benchmark_session_factory()\n",
    "\n",
    "# Disable auto-save.\n",
    "with benchmark_session_factory.constant_provider(AutoSaveFlag, False):\n",
    "    session = benchmark_session_factory[BenchmarkSession]\n",
    "\n",
    "Pretty(session, max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75c9c3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here is the simple use case of ``session.run``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9eaaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(x: float) -> float:\n",
    "    from time import sleep\n",
    "    import random\n",
    "    sleep(x)\n",
    "    sleep(random.random()*0.1)  # noqa: S311\n",
    "\n",
    "    return x\n",
    "\n",
    "session.run(test_func, 0.1)\n",
    "Pretty(session.report.measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63a66a",
   "metadata": {},
   "source": [
    "The ``session.run`` passes all arguments to ``session.runner`` and append its result into the ``session.report``.\n",
    "\n",
    "You can use ``session.configure`` to temporarily update configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e056929",
   "metadata": {},
   "outputs": [],
   "source": [
    "with session.configure(iterations=2):  # Run the benchmark twice.\n",
    "    session.run(test_func, 0.2)\n",
    "\n",
    "Pretty(session.report.measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d9fb3",
   "metadata": {},
   "source": [
    "Note that each iteration appends the result separately, instead of deriving average results and save the number of iteration together.\n",
    "\n",
    "It is because these tools are intended for time-consuming tests, in the scale of minutes and hours.\n",
    "\n",
    "If you need multi-thousands iterations similar to ``timeit``, you can write a special ``runner`` to do so.\n",
    "\n",
    "See [Exercies: TimeIt Runner](#Exercise:-TimeIt-Runner) as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffae990",
   "metadata": {},
   "source": [
    "### BenchmarkRunner\n",
    "\n",
    "The ``BenchmarkRunner`` should be a callable that returns a ``SingleRunReport``,\n",
    "that can be appended to the ``BenchmarkReport``.\n",
    "\n",
    "Here is the simple use case of the runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26671cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.runner import BenchmarkRunner\n",
    "runner = benchmark_session_factory[BenchmarkRunner]  # SimpleRunner\n",
    "\n",
    "single_report = runner(test_func, 0.1)\n",
    "Pretty(single_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0313f",
   "metadata": {},
   "source": [
    "``BenchmarkRunner`` is meant to be customized for various purposes and more complicated benchmarks.\n",
    "\n",
    "See ``tests/prototypes/prototype_mini.py`` and ``tests/prototypes/prototype_test.py`` for more complicated use-cases.\n",
    "\n",
    "Here is a simple exercise of customizing runners.\n",
    "\n",
    "#### Exercise: TimeIt Runner\n",
    "\n",
    "If you want to benchmark more than hundreds of iterations on the same target,\n",
    "it might not be ideal to append each result to the report.\n",
    "\n",
    "Let's write a runner that works with ``timeit``.\n",
    "\n",
    "It should also have ``iterations`` in the report.\n",
    "\n",
    "Since it is not part of arguments of the target function, it would better be added in the ``measurements``.\n",
    "\n",
    "Note that all measurement types need to have ``value`` and ``unit`` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.runner import BenchmarkRunner, SingleRunReport, TimeMeasurement, BenchmarkResult, BenchmarkTargetName\n",
    "from typing import Callable, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Iterations:\n",
    "    value: int\n",
    "    unit: str = 'counts'\n",
    "\n",
    "\n",
    "# Extended benchmark result container.\n",
    "@dataclass\n",
    "class TimeItBenchmarkResult(BenchmarkResult):\n",
    "    iterations: Optional[Iterations] = None\n",
    "\n",
    "\n",
    "# Customized benchmark runner.\n",
    "class TimeItRunner(BenchmarkRunner):\n",
    "    def __call__(self, func: Callable, iterations: int, **kwargs) -> SingleRunReport:\n",
    "        from functools import partial\n",
    "        from timeit import timeit\n",
    "\n",
    "        target = partial(func, **kwargs)\n",
    "        result = timeit(target, number=iterations)\n",
    "\n",
    "        return SingleRunReport(\n",
    "            callable_name=BenchmarkTargetName(func.__name__),\n",
    "            arguments=kwargs,\n",
    "            benchmark_result=TimeItBenchmarkResult(\n",
    "                TimeMeasurement(result, 's'),\n",
    "                iterations = Iterations(iterations),\n",
    "            ),\n",
    "            output=target(),\n",
    "        )\n",
    "\n",
    "\n",
    "# Build the benchmark session with the customized runner and run the tests.\n",
    "with benchmark_session_factory.temporary_provider(BenchmarkRunner, TimeItRunner):\n",
    "    timeit_session = benchmark_session_factory[BenchmarkSession]\n",
    "    timeit_session.configurations.auto_save = AutoSaveFlag(False)\n",
    "\n",
    "\n",
    "timeit_session.run(test_func, 100, x=0.001)\n",
    "Pretty(timeit_session.report.measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff7105",
   "metadata": {},
   "source": [
    "### Benchmark Report\n",
    "\n",
    "The ``report`` is a container of benchmark results.\n",
    "\n",
    "``BenchmarkReport`` should have ``append`` method that receives a ``SingleRunReport`` and save in itself.\n",
    "\n",
    "It is not expected to be customized so often so it has its own implementation.\n",
    "\n",
    "It has four dataclass fields, ``environment``, ``target_names``, ``measurements`` and ``arguments``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90908544",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pretty(session.report, max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d571993",
   "metadata": {},
   "source": [
    "``environment`` is a static field that has information of the hardware it is running on,\n",
    "and other fields contain benchmark results.\n",
    "\n",
    "``target_names``, ``measurements`` and ``arguments`` are similar to ``Series`` or ``DataFrame`` of ``pandas``.\n",
    "\n",
    "It is for exporting the report as a ``pandas.DataFrame``.\n",
    "The exported ``pandas.DataFrame`` will be then converted to\n",
    "``scipp.Dataset`` for further visualization with ``plopp``.\n",
    "\n",
    "Here is the example of the full contents of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82234cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the benchmark report.\n",
    "Pretty(session.report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e05d2",
   "metadata": {},
   "source": [
    "### Benchmark File Manager\n",
    "\n",
    "``BenchmarkFileManager.save`` should receive a ``BenchmarkReport`` and save it.\n",
    "\n",
    "It is also not expected to be customized very often.\n",
    "\n",
    "By default, it saves a result under ``.benchmarks/`` directory as a json file.\n",
    "\n",
    "If you want to choose a different directory to save the benchmark result, replace the provider of ``BenchmarkRootDir``.\n",
    "\n",
    "To avoid unnecessary file handling in the example, we will define a context manager that temporarily saves the results in the current directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def temporary_save(*sessions):\n",
    "    import os\n",
    "    original_file_path = {\n",
    "        i_session: session.file_manager.file_path\n",
    "        for i_session, session in enumerate(sessions)\n",
    "    }\n",
    "\n",
    "    for i_session, session in enumerate(sessions):\n",
    "        session.file_manager.file_path = Path('./result-%d.json' % i_session)\n",
    "        session.save()\n",
    "    yield None\n",
    "\n",
    "    for i_session, session in enumerate(sessions):\n",
    "        os.remove(session.file_manager.file_path)\n",
    "        session.file_manager.file_path = original_file_path[i_session]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42ac56",
   "metadata": {},
   "source": [
    "## Benchmark Result Loader\n",
    "\n",
    "Benchmark loader, ``tests.benchmarks.loader`` can reconstruct ``BenchmarkReport(ReportTemplate)`` from a saved json file.\n",
    "``ReportTemplate`` can be exported as a ``pandas.DataFrame``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "from tests.benchmarks.loader import reconstruct_report\n",
    "\n",
    "Pretty(reconstruct_report(asdict(session.report)), max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880627a",
   "metadata": {},
   "source": [
    "You can merge multiple reports into one data frame.\n",
    "\n",
    "Let's merge reports from ``session`` and ``timeit_session``.\n",
    "\n",
    "Note that ``NaN`` will be filled, if there is a missing columns in the report compared to other reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553459ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.loader import merge_measurements\n",
    "\n",
    "df = merge_measurements({\n",
    "    'simple': reconstruct_report(asdict(session.report)),\n",
    "    'timeit': reconstruct_report(asdict(timeit_session.report)),\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7d744",
   "metadata": {},
   "source": [
    "You can also easily load and merge all reports from a single directory like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from beamlime.constructors import Factory\n",
    "from tests.benchmarks.loader import loading_providers, MergedMeasurementsDF, BenchmarkRootDir\n",
    "\n",
    "result_factory = Factory(loading_providers)\n",
    "\n",
    "with temporary_save(session):\n",
    "    with result_factory.constant_provider(BenchmarkRootDir, Path('./')):\n",
    "        # Replace './' with the path to the directory containing the saved results.\n",
    "        df = result_factory[MergedMeasurementsDF]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69782d32",
   "metadata": {},
   "source": [
    "Let's run more tests for visualization example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with session.configure(iterations=3):\n",
    "    for i in range(4):\n",
    "        session.run(test_func, 0.1+i*0.1)\n",
    "\n",
    "with temporary_save(session):\n",
    "    with result_factory.constant_provider(BenchmarkRootDir, Path('./')):\n",
    "        # Replace './' with the path to the directory containing the saved results.\n",
    "        df = result_factory[MergedMeasurementsDF]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5cd2a3",
   "metadata": {},
   "source": [
    "## Benchmark Result Visualization\n",
    "\n",
    "There are also helpers for visualizing results. <br>\n",
    "It was much easier to use ``scipp`` and ``plopp`` so we will convert the ``pandas.DataFrame`` that we loaded to ``scipp.Dataset``. <br>\n",
    "The data frame column name has unit in the bracket, that can be parsed by ``scipp.compat.pandas_compat.parse_bracket_header``. <br>\n",
    "\n",
    "The measurement data is ``time`` column, so other columns will be coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73126aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipp.compat.pandas_compat import from_pandas, parse_bracket_header\n",
    "\n",
    "df.drop(columns=['environment'], inplace=True)  # Remove unnecessary columns.\n",
    "ds = from_pandas(df, header_parser=parse_bracket_header, data_columns='time')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c72458",
   "metadata": {},
   "source": [
    "Now we can easily convert ``time [s]`` to ``frequency [Hz]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['frequency'] = 1 / ds['time']\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9ae38",
   "metadata": {},
   "source": [
    "And there is a helper to calculate average value of data per bin as well as sample variance.\n",
    "\n",
    "First, we will bin the data per ``x``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned = ds['time'].group('x')\n",
    "binned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08972d6d",
   "metadata": {},
   "source": [
    "And you can use the helper to calculate average values and sample variance per bin.\n",
    "\n",
    "It should have the same shape as the binned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4383f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.calculations import sample_mean_per_bin, sample_variance_per_bin\n",
    "\n",
    "da = sample_mean_per_bin(binned)\n",
    "da.variances = sample_variance_per_bin(binned).values\n",
    "\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137502be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plopp as pp\n",
    "\n",
    "plot = pp.plot({'all-tests': da}, grid=True, title='Dummy function benchmark result.')\n",
    "plot.ax.set_ylim(0.05, 0.55)\n",
    "plot.ax.set_ylabel('Time [s]')\n",
    "plot.ax.set_xlabel('Parameter `x`')\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46331a",
   "metadata": {},
   "source": [
    "The sample variance will make it easier to compare different groups of results. <br>\n",
    "``sample_variance`` function was used to calculate sample variance per bin. (In ``tests.benchmarks.calculations`` module.) <br>\n",
    "It uses the following equation: <br>\n",
    "$$\n",
    "\\text{sample variance} = \\frac{\\sum{(x_i - \\bar{x})^2}}{n-1}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$x_i = \\text{each value of data (benchmark measurement)}$$\n",
    "$$\\bar{x} = \\text{mean value of all } x$$\n",
    "$$n = \\text{number of data points}$$\n",
    "\n",
    "Since the degree of freedom is $n-1$, it returns ``NaN`` if there are not enough data ($< 2$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c85aa",
   "metadata": {},
   "source": [
    "## Benchmark Session as a Pytest fixture.\n",
    "\n",
    "You can use a benchmark session as a pytest fixture.\n",
    "See ``conftest.py`` under ``tests`` for available pytest flags.\n",
    "\n",
    "The following example is how to write a fixture.\n",
    "\n",
    "If the fixture has a scope of ``function``, each result will be saved in a new file in this example.\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "from tests.benchmarks.runner import BenchmarkSession, SimpleRunner, create_benchmark_runner_factory, BenchmarkRunner\n",
    "from typing import Generator, Any\n",
    "factory = create_benchmark_runner_factory()\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def benchmark_session() -> Generator[BenchmarkSession, Any, Any]:\n",
    "    with factory.temporary_provider(BenchmarkRunner, SimpleRunner):\n",
    "        session = factory[BenchmarkSession]\n",
    "        yield session\n",
    "        # Save when the pytest session is over.\n",
    "        session.save()\n",
    "\n",
    "def a_function_you_want_to_test() -> None:\n",
    "    ...\n",
    "\n",
    "\n",
    "def test_prototype_benchmark(benchmark_session: BenchmarkSession) -> None:\n",
    "    with benchmark_session.configure(iterations=100):  # Run the test 100 times.\n",
    "        benchmark_session.run(a_function_you_want_to_test)\n",
    "\n",
    "    # Save when a single test is over.\n",
    "    benchmark_session.save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce7ca92",
   "metadata": {},
   "source": [
    "## Customizing Benchmark Providers.\n",
    "\n",
    "You can customize the benchmark tools by replacing providers.\n",
    "\n",
    "For example, you can customize a benchmark report by adding more fields or removing unnecessary fields.\n",
    "\n",
    "Let's use a smaller subset, ``HardwareSpec`` for exercises.\n",
    "\n",
    "### Exercise 1: Add an extra field into ``HardwareSpec``.\n",
    "To have an extra field, the customized type of ``HardwareSpec`` should be\n",
    "\n",
    "1. A subclass of the original type ``HardwareSpec``\n",
    "2. Decorated as a ``dataclass``\n",
    "\n",
    "(1) is to keep the child class compatible with ``HardwareSpec`` as a provider of ``HardwareSpec``,\n",
    "(It only allows if it is a subclass or itself.) and\n",
    "(2) is to keep the child class compatible with ``asdict`` of ``dataclass`` in the ``BenchmarkReport``.\n",
    "\n",
    "See the following example of implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.environments import HardwareSpec, env_providers\n",
    "from beamlime.constructors import Factory\n",
    "from dataclasses import dataclass\n",
    "from typing import NewType\n",
    "from copy import copy\n",
    "\n",
    "minimum_env_providers = copy(env_providers)\n",
    "\n",
    "TMI = NewType(\"TMI\", str)\n",
    "\n",
    "# This class can't be decorated as a provider since it is a provider of its parent type.\n",
    "@dataclass\n",
    "class MyHardwareSpec(HardwareSpec):\n",
    "    extra_info: TMI = TMI(\"A little more information.\") # noqa: RUF009\n",
    "\n",
    "# ``MyHardwareSpec`` should be explicitly registered as a provider.\n",
    "minimum_env_providers.pop(HardwareSpec)\n",
    "minimum_env_providers[HardwareSpec] = MyHardwareSpec\n",
    "custom_env_factory = Factory(minimum_env_providers)\n",
    "\n",
    "Pretty(custom_env_factory[HardwareSpec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf5a59b",
   "metadata": {},
   "source": [
    "### Exercise 2: Remove ``CPUSpec`` from ``HardwareSpec``.\n",
    "If you want to ``remove``/``exclude`` a field, there are more steps needed than just overwriting an existing ones.\n",
    "\n",
    "Here are the options.\n",
    "\n",
    "1. Annotate the field as ``Optional`` and remove the provider.\n",
    "   Please note that the field will be populated if there is a provider in the provider group.\n",
    "   The field will be set as ``None`` so it is not completely removed.\n",
    "2. Replace the provider with another class without the field.\n",
    "   It should not inherit the original class.\n",
    "   Note that the users of this class also need to be updated in this case.\n",
    "\n",
    "See the following examples of removing ``CPUSpec`` from ``HardwareSpec``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cebb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.environments import CPUSpec, BenchmarkEnvironment\n",
    "from typing import Optional\n",
    "\n",
    "# 1. Annotate the field as ``Optional`` and remove the provider.\n",
    "\n",
    "optional_env_providers = copy(env_providers)\n",
    "\n",
    "optional_env_providers.pop(CPUSpec)\n",
    "\n",
    "@dataclass\n",
    "class LessHardwareSpec(HardwareSpec):\n",
    "    cpu_spec: Optional[CPUSpec] = None\n",
    "\n",
    "\n",
    "optional_env_providers.pop(HardwareSpec)\n",
    "optional_env_providers[HardwareSpec] = LessHardwareSpec\n",
    "\n",
    "Pretty(Factory(optional_env_providers)[BenchmarkEnvironment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import make_dataclass, fields\n",
    "\n",
    "# 2. Replace the provider with another class without the field.\n",
    "replaced_env_providers = copy(env_providers)\n",
    "\n",
    "RHS = make_dataclass(\n",
    "    \"ReplacedHardwareSpec\",\n",
    "    [(field.name, field.type, field) for field in fields(HardwareSpec) if field.type != CPUSpec]\n",
    ")\n",
    "\n",
    "@replaced_env_providers.provider\n",
    "class ReplacedHardwareSpec(RHS):\n",
    "    ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReplacedBenchmarkEnvironment(BenchmarkEnvironment):\n",
    "    hardware_spec: ReplacedHardwareSpec  # Hardware spec is overwritten.\n",
    "\n",
    "\n",
    "replaced_env_providers.pop(BenchmarkEnvironment)\n",
    "replaced_env_providers[BenchmarkEnvironment] = ReplacedBenchmarkEnvironment\n",
    "\n",
    "Pretty(Factory(replaced_env_providers)[BenchmarkEnvironment])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda4254",
   "metadata": {},
   "source": [
    "### Exercise 3: Empty Ancestor for Easy Customization.\n",
    "\n",
    "If you often need to customized providers, consider updating the original one.\n",
    "If you don't want to include/remove fields from the original one but still have to customize them often,\n",
    "consider adding an extra ancestor class that contains nothing and annotate the frequently customized fields of its users.\n",
    "\n",
    "Not all types are implemented this way from the beginning to avoid too many layers of inheritance and complicated code base.\n",
    "\n",
    "For example, ``PrototypeRunner`` is expected to be replaced often.\n",
    "``PrototypeRunner`` works as a ``Protocol`` or an ``Interface`` and the ``BenchmarkSession.runner`` is annotated with this type.\n",
    "So users need to implement a subclass and explicitly set it as a provider of the ``PrototypeRunner``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beamlime.constructors import ProviderGroup\n",
    "import os\n",
    "\n",
    "# Update the original types as following.\n",
    "env_providers = ProviderGroup()\n",
    "\n",
    "OsInfo = NewType(\"OsInfo\", str)\n",
    "env_providers[OsInfo] = lambda: OsInfo(os.uname().version)\n",
    "\n",
    "class HardwareSpec:\n",
    "    ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DefaultHardwareSpec(HardwareSpec):\n",
    "    os_info: OsInfo\n",
    "\n",
    "\n",
    "# ``HardwareSpec`` needs to be explicitly registered as a provider of ``HardwareSpecAncestor``.\n",
    "env_providers[HardwareSpec] = DefaultHardwareSpec\n",
    "\n",
    "@env_providers.provider\n",
    "@dataclass\n",
    "class BenchmarkEnvironment:\n",
    "    hardware_spec: HardwareSpec\n",
    "\n",
    "\n",
    "Pretty(Factory(env_providers)[BenchmarkEnvironment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the provider of ``HardwareSpecAncestor``.\n",
    "updated_providers = copy(env_providers)\n",
    "\n",
    "@dataclass\n",
    "class EasilyUpdatedHardwareSpec(HardwareSpec):\n",
    "    extra_info: TMI = TMI(\"A little more information.\")  # noqa: RUF009\n",
    "\n",
    "# Then it is much easier to replace the provider,\n",
    "# Since the user class doesn't need to be updated.\n",
    "updated_providers.pop(HardwareSpec)\n",
    "updated_providers[HardwareSpec] = EasilyUpdatedHardwareSpec\n",
    "\n",
    "Pretty(Factory(updated_providers)[BenchmarkEnvironment])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
